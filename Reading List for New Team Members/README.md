# Reading List for New Team Members

Here is a list of articles to get you started with LLM inference and optimization.

1.  [Understanding GPU for Inference in LLMs](https://adaline.ai/blog/understanding-gpu-for-inference-in-llms)
2.  [A Complete Guide to LLM Inference Servers](https://medium.com/@subhashdasyam/a-complete-guide-to-llm-inference-servers-vllm-tensorrt-llm-and-more-part-1-46941006a18)
3.  [Best practices for optimizing LLM inference on GKE](https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-optimizing-llm-inference-on-gke)
4.  [LLM Inference at Scale: A Guide to Optimizing Performance, Cost, and Observability](https://www.anyscale.com/blog/llm-inference-at-scale-a-guide-to-optimizing-performance-cost-and-observability)
5.  [A Gentle Introduction to LLM Serving with vLLM](https://medium.com/@joaolages/a-gentle-introduction-to-llm-serving-with-vllm-3ab1a4441074)